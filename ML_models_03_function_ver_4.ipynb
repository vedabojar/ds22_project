{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6f442f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, VotingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn.neighbors\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.svm import LinearSVR \n",
    "from sklearn import neighbors\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import PolynomialFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebca069f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Mae</th>\n",
       "      <th>Mse</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Reg</td>\n",
       "      <td>0.576316</td>\n",
       "      <td>0.572359</td>\n",
       "      <td>0.756544</td>\n",
       "      <td>0.294815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVR</td>\n",
       "      <td>0.696816</td>\n",
       "      <td>1.156140</td>\n",
       "      <td>1.075240</td>\n",
       "      <td>-0.424444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Descion Tree</td>\n",
       "      <td>0.622455</td>\n",
       "      <td>0.643793</td>\n",
       "      <td>0.802367</td>\n",
       "      <td>0.206802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DT bag</td>\n",
       "      <td>0.589913</td>\n",
       "      <td>0.610591</td>\n",
       "      <td>0.781403</td>\n",
       "      <td>0.247710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DT vot</td>\n",
       "      <td>0.622455</td>\n",
       "      <td>0.643793</td>\n",
       "      <td>0.781403</td>\n",
       "      <td>0.206802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.574472</td>\n",
       "      <td>0.570548</td>\n",
       "      <td>0.755347</td>\n",
       "      <td>0.297045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.717206</td>\n",
       "      <td>0.869494</td>\n",
       "      <td>0.932467</td>\n",
       "      <td>-0.071276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.637596</td>\n",
       "      <td>0.686831</td>\n",
       "      <td>0.828753</td>\n",
       "      <td>0.153777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Polynomial</td>\n",
       "      <td>0.580622</td>\n",
       "      <td>0.590209</td>\n",
       "      <td>0.768250</td>\n",
       "      <td>0.295846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model       Mae       Mse      RMSE        R2\n",
       "0    Linear Reg  0.576316  0.572359  0.756544  0.294815\n",
       "1           SVR  0.696816  1.156140  1.075240 -0.424444\n",
       "2  Descion Tree  0.622455  0.643793  0.802367  0.206802\n",
       "3        DT bag  0.589913  0.610591  0.781403  0.247710\n",
       "4        DT vot  0.622455  0.643793  0.781403  0.206802\n",
       "5            RF  0.574472  0.570548  0.755347  0.297045\n",
       "6           KNN  0.717206  0.869494  0.932467 -0.071276\n",
       "7         Lasso  0.637596  0.686831  0.828753  0.153777\n",
       "8    Polynomial  0.580622  0.590209  0.768250  0.295846"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_models(df, target_col):\n",
    "    \n",
    "    # assuming your dataframe is named 'df' and our column we want to predict is 'Rating' column\n",
    "    X = df.drop(target_col, axis=1)\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Splitting the df to train, validation, and test sets\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.18, random_state=42)\n",
    "    \n",
    "    # print the shape of the datasets\n",
    "    #print(f\"Shape of X: {X.shape}\")\n",
    "    #print(f\"Shape of X_train: {X_train.shape}\")\n",
    "    #print(f\"Shape of X_val: {X_val.shape}\")\n",
    "    #print(f\"Shape of X_test: {X_test.shape}\")\n",
    "    \n",
    "    # Model 1 - Linear Regression\n",
    "\n",
    "    ## Errorif y_type in [\"binary\", \"multiclass\"]:  Because I tried to predict y = df[['Rating', 'Profit_inf']]at the same time. \n",
    "    #ValueError: continuous-multioutput is not supported\n",
    "\n",
    "    # scaling w RobustScaler object and fit to training data\n",
    "    scaler = RobustScaler()\n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    # apply the scaler to both the training and testing data\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "    y_pred =lr.predict(X_test_scaled)\n",
    "\n",
    "    # evaluate the model performance using mean absolute error and mean squared error and RMSE\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    RMSE = np.sqrt(mse)\n",
    "    R2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    lin_reg = (mae, mse, RMSE, R2)\n",
    "    \n",
    "    # Model 2 - SVR (Support Vector Machine Regressor)\n",
    "\n",
    "    ##SVR performs better on regression problems whereas SVM on classification problems. Therfore we continue with SVR \n",
    "\n",
    "    #Scaling the data for SVM model\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    #fitting the training data for SVM model\n",
    "    svm_reg= LinearSVR(epsilon=1.5)\n",
    "\n",
    "    svm_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "    y_pred = svm_reg.predict(X_test_scaled)\n",
    "\n",
    "    # Performance metrics for SVR\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    RMSE = np.sqrt(mse)\n",
    "    R2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    svr = (mae, mse, RMSE, R2)\n",
    "    \n",
    "    # Model 3 - Decision Trees\n",
    "\n",
    "    #Voting and Bagging regressors on Ensemble methods help the models to reduce overfitting therefore we apply with Decision Tree\n",
    "\n",
    "    # Define a decision tree model\n",
    "    dtree = DecisionTreeRegressor(max_depth=3)\n",
    "\n",
    "    dtree.fit(X_train, y_train) \n",
    "\n",
    "    # Define a bagging regressor with decision tree models\n",
    "    bagging_model = BaggingRegressor(base_estimator=dtree, n_estimators=10, random_state=42)\n",
    "\n",
    "    #Approach is to use the same training algorithm for every predictor and train them on different random subsets of the training set\n",
    "\n",
    "    # Define a voting regressor with decision tree models\n",
    "    voting_model = VotingRegressor([('tree1', dtree), ('tree2', dtree), ('tree3', dtree)])\n",
    "\n",
    "    # Fit the models on the training data\n",
    "    dtree.fit(X_train, y_train)\n",
    "    bagging_model.fit(X_train, y_train)\n",
    "    voting_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    y_pred_dtree = dtree.predict(X_test)\n",
    "    y_pred_bagging = bagging_model.predict(X_test)\n",
    "    y_pred_voting = voting_model.predict(X_test)\n",
    "    \n",
    "    mae_dtree = mean_absolute_error(y_test, y_pred_dtree)\n",
    "    mae_bagging = mean_absolute_error(y_test, y_pred_bagging)\n",
    "    mae_voting = mean_absolute_error(y_test, y_pred_voting)\n",
    "\n",
    "    # Calculate the mean squared error of the predictions\n",
    "    mse_dtree = mean_squared_error(y_test, y_pred_dtree)\n",
    "    mse_bagging = mean_squared_error(y_test, y_pred_bagging)\n",
    "    mse_voting = mean_squared_error(y_test, y_pred_voting)\n",
    "\n",
    "    rmse_dtree = np.sqrt(mse_dtree)\n",
    "    rmse_bag= np.sqrt(mse_bagging)\n",
    "    rmse_voting = np.sqrt(mse_voting)\n",
    "\n",
    "    R2_dtree = r2_score(y_test, y_pred_dtree)\n",
    "    R2_bagging = r2_score(y_test, y_pred_bagging)\n",
    "    R2_voting = r2_score(y_test, y_pred_voting)\n",
    "\n",
    "    tree = (mae_dtree, mse_dtree, rmse_dtree, R2_dtree)\n",
    "    tree_bag = (mae_bagging, mse_bagging, rmse_bag, R2_bagging)\n",
    "    tree_vot = (mae_voting, mse_voting, rmse_bag, R2_voting)\n",
    "\n",
    "    # Cross validation , cv on Decision tree model \n",
    "    \n",
    "    # Model 4 - Random Forest\n",
    "\n",
    "    # Initialize the model\n",
    "    rf = RandomForestRegressor(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=42)\n",
    "\n",
    "    #training model to Rfor.\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = rf.predict(X_test)\n",
    "\n",
    "    # evaluate the model performance using mean absolute error and mean squared error and RMSE\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    rf = (mae, mse, rmse, r2)\n",
    "          \n",
    "    # Model 5 - K-nearest neighbors\n",
    "    \n",
    "    # Train the KNN model\n",
    "    k = 5\n",
    "    knn = neighbors.KNeighborsRegressor(n_neighbors =k)\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    scaler=RobustScaler()\n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = knn.predict(X_test)\n",
    "\n",
    "    #Model performance metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    knn = (mae, mse, rmse, r2)\n",
    "\n",
    "    #if model performs good and generalises good , why negativ R2??\n",
    "    \n",
    "    # Model 6 - Lasso Regression\n",
    "    # Lasso regression can help with feature selection by shrinking the coefficients of less important features to zero.\n",
    "\n",
    "    # scaling the data\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # initialize lasso regression model\n",
    "    lasso = Lasso(alpha=0.1)\n",
    "\n",
    "    # fit the model to the training data\n",
    "    lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # make predictions on the test set\n",
    "    y_pred = lasso.predict(X_test_scaled)\n",
    "\n",
    "    # evaluate the model performance using mean absolute error and mean squared error and RMSE\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    RMSE = np.sqrt(mse)\n",
    "    R2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    lasso_reg = (mae, mse, RMSE, R2)\n",
    "    \n",
    "    # Model 7 - Polynomial\n",
    "    # Drop target column in df\n",
    "    X = df.drop(target_col, axis=1)\n",
    "    y = df[target_col]\n",
    "\n",
    "    # Convert X to a numpy array before reshaping\n",
    "    X_array = X.values.reshape(-1, 1)\n",
    "\n",
    "    poly= PolynomialFeatures(degree=2, include_bias= False)\n",
    "\n",
    "    X_poly = poly.fit_transform(X_array)\n",
    "\n",
    "    #print(X_poly.shape)\n",
    "\n",
    "    #splitting train, test val_set:\n",
    "    # train set=0.8, test set=0.2, val set= 0.8*0.25= 0.2\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "    # scaling w RobustScaler object and fit only to training data\n",
    "    scaler = RobustScaler()\n",
    "\n",
    "    # Apply scaler to training, validation, and test data\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "    lr.intercept_, lr.coef_ \n",
    "\n",
    "    y_pred_val = lr.predict(X_val_scaled)\n",
    "    y_pred_test = lr.predict(X_test_scaled)\n",
    "\n",
    "    #predictions on val set\n",
    "    mse = mean_squared_error(y_val, y_pred_val)\n",
    "    RMSE = np.sqrt(mse)\n",
    "\n",
    "    #print(\"Validation set MSE: {:.2f}\".format(mse))\n",
    "    #print(f'RMSE: {RMSE}')\n",
    "\n",
    "    #predictions on test set\n",
    "    mse = mean_squared_error(y_test, y_pred_test)\n",
    "    RMSE = np.sqrt(mse)\n",
    "    \n",
    "    # Calculate the Mean Absolute Error (MAE)\n",
    "    mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    #print(\"Mean Absolute Error:\", mae)\n",
    "\n",
    "    # Calculate the R-squared (R2)\n",
    "    r2 = r2_score(y_test, y_pred_test)\n",
    "    #print(\"R-squared:\", r2)\n",
    "\n",
    "    #print(\"Test set MSE: {:.2f}\".format(mse))\n",
    "    #print(f'RMSE: {RMSE}')\n",
    "\n",
    "    # Visualising the Polynomial Regression: edit\n",
    "\n",
    "    # got error code \n",
    "    #plt.scatter(X_array, y, color = 'blue')\n",
    "\n",
    "    #plt.plot(X, lg.predict(poly.fit_transform(X)), color = 'violet')\n",
    "    #plt.title('Polynomial Regression')\n",
    "    #plt.xlabel('X')\n",
    "    #plt.ylabel('ploy_predicted')\n",
    "    \n",
    "    pol = (mae, mse, RMSE, r2)\n",
    "    \n",
    "\n",
    "    models = lin_reg, svr, tree, tree_bag, tree_vot, rf, knn, lasso_reg, pol\n",
    "    # create a dictionary of data\n",
    "    headers = ['Model', 'Mae', 'Mse', 'RMSE', 'R2']\n",
    "    models_name = ['Linear Reg', 'SVR', 'Descion Tree', 'DT bag', 'DT vot', 'RF', 'KNN', 'Lasso', 'Polynomial']\n",
    "\n",
    "    data = {headers[0]: [models_name[0], models_name[1], models_name[2], models_name[3], models_name[4], models_name[5], models_name[6], models_name[7], models_name[8]],\n",
    "            headers[1]: [models[0][0], models[1][0], models[2][0], models[3][0], models[4][0], models[5][0], models[6][0], models[7][0], models[8][0]],\n",
    "            headers[2]: [models[0][1], models[1][1], models[2][1], models[3][1], models[4][1], models[5][1], models[6][1], models[7][1], models[8][1]],\n",
    "            headers[3]: [models[0][2], models[1][2], models[2][2], models[3][2], models[4][2], models[5][2], models[6][2], models[7][2], models[8][2]],\n",
    "            headers[4]: [models[0][3], models[1][3], models[2][3], models[3][3], models[4][3], models[5][3], models[6][3], models[7][3], models[8][3]]}\n",
    "\n",
    "    # create a DataFrame from the dictionary\n",
    "    df_models = pd.DataFrame(data)\n",
    "    \n",
    "    return df_models\n",
    "\n",
    "# Compare models and print result    \n",
    "df=pd.read_csv('./data/mvoies_processed_noTitle.csv')\n",
    "target_col = \"Rating\"\n",
    "\n",
    "models = compare_models(df, target_col)\n",
    "models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "436c8705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped column: Year\n",
      "          Model       Mae       Mse      RMSE        R2\n",
      "0    Linear Reg  0.575617  0.572372  0.756553  0.294798\n",
      "1           SVR  0.690164  1.104761  1.051076 -0.361142\n",
      "2  Descion Tree  0.622455  0.643793  0.802367  0.206802\n",
      "3        DT bag  0.589913  0.610591  0.781403  0.247710\n",
      "4        DT vot  0.622455  0.643793  0.781403  0.206802\n",
      "5            RF  0.574896  0.570323  0.755197  0.297324\n",
      "6           KNN  0.717206  0.869494  0.932467 -0.071276\n",
      "7         Lasso  0.637596  0.686831  0.828753  0.153777\n",
      "8    Polynomial  0.582025  0.591586  0.769147  0.294202\n",
      "Dropped column: Month\n",
      "          Model       Mae       Mse      RMSE        R2\n",
      "0    Linear Reg  0.576786  0.572053  0.756342  0.295191\n",
      "1           SVR  0.695252  1.147157  1.071054 -0.413376\n",
      "2  Descion Tree  0.622455  0.643793  0.802367  0.206802\n",
      "3        DT bag  0.589384  0.608933  0.780342  0.249753\n",
      "4        DT vot  0.622455  0.643793  0.780342  0.206802\n",
      "5            RF  0.574866  0.571130  0.755731  0.296329\n",
      "6           KNN  0.717206  0.869494  0.932467 -0.071276\n",
      "7         Lasso  0.637596  0.686831  0.828753  0.153777\n",
      "8    Polynomial  0.580430  0.588937  0.767423  0.297363\n",
      "Dropped column: Runtime\n",
      "          Model       Mae       Mse      RMSE        R2\n",
      "0    Linear Reg  0.577078  0.598138  0.773394  0.263052\n",
      "1           SVR  0.689101  1.312614  1.145694 -0.617231\n",
      "2  Descion Tree  0.627121  0.677019  0.822812  0.165865\n",
      "3        DT bag  0.597031  0.634149  0.796335  0.218685\n",
      "4        DT vot  0.627121  0.677019  0.796335  0.165865\n",
      "5            RF  0.593788  0.623170  0.789411  0.232212\n",
      "6           KNN  0.717206  0.869494  0.932467 -0.071276\n",
      "7         Lasso  0.645865  0.715473  0.845857  0.118488\n",
      "8    Polynomial  0.603046  0.640134  0.800084  0.236282\n",
      "Dropped column: Budget_inf\n",
      "          Model       Mae       Mse      RMSE        R2\n",
      "0    Linear Reg  0.576316  0.572359  0.756544  0.294815\n",
      "1           SVR  0.685875  1.058297  1.028736 -0.303895\n",
      "2  Descion Tree  0.622455  0.643793  0.802367  0.206802\n",
      "3        DT bag  0.591762  0.611443  0.781948  0.246660\n",
      "4        DT vot  0.622455  0.643793  0.781948  0.206802\n",
      "5            RF  0.575637  0.572424  0.756587  0.294734\n",
      "6           KNN  0.711765  0.859926  0.927322 -0.059488\n",
      "7         Lasso  0.637596  0.686831  0.828753  0.153777\n",
      "8    Polynomial  0.580622  0.590209  0.768250  0.295846\n",
      "Dropped column: Income_inf\n",
      "          Model       Mae       Mse      RMSE        R2\n",
      "0    Linear Reg  0.576316  0.572359  0.756544  0.294815\n",
      "1           SVR  0.743128  2.516695  1.586409 -2.100741\n",
      "2  Descion Tree  0.622455  0.643793  0.802367  0.206802\n",
      "3        DT bag  0.589913  0.610591  0.781403  0.247710\n",
      "4        DT vot  0.622455  0.643793  0.781403  0.206802\n",
      "5            RF  0.574340  0.571347  0.755875  0.296061\n",
      "6           KNN  0.709926  0.863249  0.929112 -0.063581\n",
      "7         Lasso  0.637597  0.686832  0.828753  0.153776\n",
      "8    Polynomial  0.580622  0.590209  0.768250  0.295846\n",
      "Dropped column: Profit_inf\n",
      "          Model       Mae       Mse      RMSE        R2\n",
      "0    Linear Reg  0.576316  0.572359  0.756544  0.294815\n",
      "1           SVR  0.780769  4.389129  2.095025 -4.407708\n",
      "2  Descion Tree  0.622455  0.643793  0.802367  0.206802\n",
      "3        DT bag  0.589346  0.611143  0.781757  0.247029\n",
      "4        DT vot  0.622455  0.643793  0.781757  0.206802\n",
      "5            RF  0.578265  0.576934  0.759562  0.289177\n",
      "6           KNN  0.726985  0.879166  0.937639 -0.083193\n",
      "7         Lasso  0.644572  0.700049  0.836689  0.137492\n",
      "8    Polynomial  0.580622  0.590209  0.768250  0.295846\n",
      "Dropped column: ROI_inf\n",
      "          Model       Mae       Mse      RMSE        R2\n",
      "0    Linear Reg  0.581635  0.570163  0.755091  0.297520\n",
      "1           SVR  0.653282  0.676398  0.822434  0.166631\n",
      "2  Descion Tree  0.626060  0.668526  0.817634  0.176330\n",
      "3        DT bag  0.606105  0.637204  0.798251  0.214921\n",
      "4        DT vot  0.626060  0.668526  0.798251  0.176330\n",
      "5            RF  0.587537  0.593348  0.770291  0.268955\n",
      "6           KNN  0.717206  0.869494  0.932467 -0.071276\n",
      "7         Lasso  0.641045  0.693378  0.832693  0.145711\n",
      "8    Polynomial  0.790749  1.101959  1.049742 -0.314703\n",
      "Dropped columns: Starts with: cert\n",
      "          Model       Mae       Mse      RMSE        R2\n",
      "0    Linear Reg  0.585272  0.582751  0.763381  0.282011\n",
      "1           SVR  0.678077  0.740534  0.860543  0.087611\n",
      "2  Descion Tree  0.622455  0.643793  0.802367  0.206802\n",
      "3        DT bag  0.589193  0.609251  0.780545  0.249361\n",
      "4        DT vot  0.622455  0.643793  0.780545  0.206802\n",
      "5            RF  0.580743  0.583563  0.763913  0.281011\n",
      "6           KNN  0.717206  0.869494  0.932467 -0.071276\n",
      "7         Lasso  0.637596  0.686831  0.828753  0.153777\n",
      "8    Polynomial  0.568415  0.560624  0.748748  0.331142\n",
      "None\n",
      "Dropped columns: Starts with: genre\n",
      "          Model       Mae       Mse      RMSE        R2\n",
      "0    Linear Reg  0.597683  0.608030  0.779763  0.250865\n",
      "1           SVR  0.677037  1.089418  1.043752 -0.342238\n",
      "2  Descion Tree  0.646709  0.677344  0.823009  0.165465\n",
      "3        DT bag  0.607223  0.632794  0.795484  0.220354\n",
      "4        DT vot  0.646709  0.677344  0.795484  0.165465\n",
      "5            RF  0.586969  0.590935  0.768723  0.271927\n",
      "6           KNN  0.717206  0.869494  0.932467 -0.071276\n",
      "7         Lasso  0.655472  0.714412  0.845229  0.119796\n",
      "8    Polynomial  0.606196  0.636613  0.797881  0.240482\n",
      "None\n",
      "Dropped columns: Starts with: contient\n",
      "          Model       Mae       Mse      RMSE        R2\n",
      "0    Linear Reg  0.584647  0.587208  0.766295  0.276519\n",
      "1           SVR  0.754887  2.310894  1.520162 -1.847180\n",
      "2  Descion Tree  0.622455  0.643793  0.802367  0.206802\n",
      "3        DT bag  0.590513  0.610835  0.781559  0.247409\n",
      "4        DT vot  0.622455  0.643793  0.781559  0.206802\n",
      "5            RF  0.575531  0.570611  0.755388  0.296968\n",
      "6           KNN  0.717206  0.869494  0.932467 -0.071276\n",
      "7         Lasso  0.637596  0.686831  0.828753  0.153777\n",
      "8    Polynomial  0.588814  0.599601  0.774339  0.284640\n",
      "None\n",
      "Dropped columns: Starts with: top_50_director\n",
      "          Model       Mae       Mse      RMSE        R2\n",
      "0    Linear Reg  0.579940  0.580753  0.762072  0.284472\n",
      "1           SVR  0.664048  0.716121  0.846239  0.117689\n",
      "2  Descion Tree  0.622540  0.653430  0.808350  0.194930\n",
      "3        DT bag  0.592186  0.613902  0.783519  0.243631\n",
      "4        DT vot  0.622540  0.653430  0.783519  0.194930\n",
      "5            RF  0.575193  0.572616  0.756714  0.294497\n",
      "6           KNN  0.717206  0.869494  0.932467 -0.071276\n",
      "7         Lasso  0.637596  0.686831  0.828753  0.153777\n",
      "8    Polynomial  0.591931  0.607789  0.779609  0.274871\n",
      "None\n",
      "Dropped columns: Starts with: top_1000_Stars\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1664\\3672504477.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'cert'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'genre'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'contient'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'top_50_director'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'top_1000_Stars'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop_and_compare_cluster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_col\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1664\\3672504477.py\u001b[0m in \u001b[0;36mdrop_and_compare_cluster\u001b[1;34m(df, col_prefix, target_col)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mdf_drop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcols_to_drop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Dropped columns:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Starts with: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcol_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mmodels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompare_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_drop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_col\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1664\\2287756884.py\u001b[0m in \u001b[0;36mcompare_models\u001b[1;34m(df, target_col)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[1;31m#training model to Rfor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m     \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[1;31m# Make predictions on the test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new_python_3_7_15\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    439\u001b[0m             trees = [\n\u001b[0;32m    440\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_more_estimators\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m             ]\n\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new_python_3_7_15\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    439\u001b[0m             trees = [\n\u001b[0;32m    440\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_more_estimators\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m             ]\n\u001b[0;32m    443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new_python_3_7_15\\lib\\site-packages\\sklearn\\ensemble\\_base.py\u001b[0m in \u001b[0;36m_make_estimator\u001b[1;34m(self, append, random_state)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[0msub\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m         \"\"\"\n\u001b[1;32m--> 158\u001b[1;33m         \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_estimator_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator_params\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new_python_3_7_15\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mclone\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[0mklass\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m     \u001b[0mnew_object_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnew_object_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mnew_object_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new_python_3_7_15\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mget_params\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    207\u001b[0m         \"\"\"\n\u001b[0;32m    208\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_param_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdeep\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"get_params\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new_python_3_7_15\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_get_param_names\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m    176\u001b[0m         parameters = [\n\u001b[0;32m    177\u001b[0m             \u001b[0mp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minit_signature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"self\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVAR_KEYWORD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         ]\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new_python_3_7_15\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    177\u001b[0m             \u001b[0mp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minit_signature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"self\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVAR_KEYWORD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m         ]\n\u001b[0;32m    181\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\new_python_3_7_15\\lib\\inspect.py\u001b[0m in \u001b[0;36mname\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2515\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_annotation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'_annotation'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2517\u001b[1;33m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2518\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2519\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('./data/mvoies_processed_noTitle.csv')\n",
    "\n",
    "def drop_and_compare_single(df, prefixes, target_col):\n",
    "    for prefix in prefixes:\n",
    "        for col in df.columns:\n",
    "            if col.startswith(prefix):\n",
    "                df_drop = df.drop(col, axis=1)\n",
    "                print(\"Dropped column:\", col)\n",
    "                models = compare_models(df_drop, target_col=target_col)\n",
    "                print(models)\n",
    "\n",
    "def drop_and_compare_cluster(df, col_prefix, target_col):\n",
    "    cols_to_drop = [col for col in df.columns if col.startswith(col_prefix)]\n",
    "    df_drop = df.drop(cols_to_drop, axis=1)\n",
    "    print(\"Dropped columns:\", \"Starts with: \" + col_prefix)\n",
    "    models = compare_models(df_drop, target_col=target_col)\n",
    "    print(models)\n",
    "\n",
    "target_col = \"Rating\"\n",
    "\n",
    "# Dropping each column one by one and running the models\n",
    "drop_and_compare_single(df, ['Year', 'Month', 'Runtime', 'Budget_inf', 'Income_inf', 'Profit_inf', 'ROI_inf'], target_col)\n",
    "\n",
    "# Dropping all columns starting with prefix and running the models\n",
    "cols = ['cert', 'genre', 'contient', 'top_50_director', 'top_1000_Stars']\n",
    "for col in cols:\n",
    "    print(drop_and_compare_cluster(df, col, target_col))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ff28ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
