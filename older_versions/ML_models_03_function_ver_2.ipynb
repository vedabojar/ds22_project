{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6f442f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, VotingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sklearn.neighbors\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.svm import LinearSVR \n",
    "from sklearn import neighbors\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import PolynomialFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebca069f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88690, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Mae</th>\n",
       "      <th>Mse</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Reg</td>\n",
       "      <td>0.569434</td>\n",
       "      <td>0.562488</td>\n",
       "      <td>0.749992</td>\n",
       "      <td>0.328918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVR</td>\n",
       "      <td>0.701876</td>\n",
       "      <td>1.313846</td>\n",
       "      <td>1.146231</td>\n",
       "      <td>-0.567496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Descion Tree</td>\n",
       "      <td>0.615102</td>\n",
       "      <td>0.644640</td>\n",
       "      <td>0.802895</td>\n",
       "      <td>0.230906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DT bag</td>\n",
       "      <td>0.589959</td>\n",
       "      <td>0.603315</td>\n",
       "      <td>0.776733</td>\n",
       "      <td>0.28021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DT vot</td>\n",
       "      <td>0.615102</td>\n",
       "      <td>0.644640</td>\n",
       "      <td>0.776733</td>\n",
       "      <td>0.230906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.573308</td>\n",
       "      <td>0.554279</td>\n",
       "      <td>0.744499</td>\n",
       "      <td>0.338712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.740939</td>\n",
       "      <td>0.931164</td>\n",
       "      <td>0.964968</td>\n",
       "      <td>-0.110934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.654185</td>\n",
       "      <td>0.717045</td>\n",
       "      <td>0.846785</td>\n",
       "      <td>0.144522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Polynomial</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0.590209</td>\n",
       "      <td>0.768250</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model       Mae       Mse      RMSE        R2\n",
       "0    Linear Reg  0.569434  0.562488  0.749992  0.328918\n",
       "1           SVR  0.701876  1.313846  1.146231 -0.567496\n",
       "2  Descion Tree  0.615102  0.644640  0.802895  0.230906\n",
       "3        DT bag  0.589959  0.603315  0.776733   0.28021\n",
       "4        DT vot  0.615102  0.644640  0.776733  0.230906\n",
       "5            RF  0.573308  0.554279  0.744499  0.338712\n",
       "6           KNN  0.740939  0.931164  0.964968 -0.110934\n",
       "7         Lasso  0.654185  0.717045  0.846785  0.144522\n",
       "8    Polynomial       N/A  0.590209  0.768250       N/A"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_models(df, target_col):\n",
    "    \n",
    "    # assuming your dataframe is named 'df' and our column we want to predict is 'Rating' column\n",
    "    X = df.drop(target_col, axis=1)\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Splitting the df to train and test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Model 1 - Linear Regression\n",
    "\n",
    "    ## Errorif y_type in [\"binary\", \"multiclass\"]:  Because I tried to predict y = df[['Rating', 'Profit_inf']]at the same time. \n",
    "    #ValueError: continuous-multioutput is not supported\n",
    "\n",
    "    # scaling w RobustScaler object and fit to training data\n",
    "    scaler = RobustScaler()\n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    # apply the scaler to both the training and testing data\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "    y_pred =lr.predict(X_test_scaled)\n",
    "\n",
    "    # evaluate the model performance using mean absolute error and mean squared error and RMSE\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    RMSE = np.sqrt(mse)\n",
    "    R2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    lin_reg = (mae, mse, RMSE, R2)\n",
    "    \n",
    "    # Model 2 - SVR (Support Vector Machine Regressor)\n",
    "\n",
    "    ##SVR performs better on regression problems whereas SVM on classification problems. Therfore we continue with SVR \n",
    "\n",
    "    #Scaling the data for SVM model\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    #fitting the training data for SVM model\n",
    "    svm_reg= LinearSVR(epsilon=1.5)\n",
    "\n",
    "    svm_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "    y_pred = svm_reg.predict(X_test_scaled)\n",
    "\n",
    "    # Performance metrics for SVR\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    RMSE = np.sqrt(mse)\n",
    "    R2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    svr = (mae, mse, RMSE, R2)\n",
    "    \n",
    "    # Model 3 - Decision Trees\n",
    "\n",
    "    #Voting and Bagging regressors on Ensemble methods help the models to reduce overfitting therefore we apply with Decision Tree\n",
    "\n",
    "    # Define a decision tree model\n",
    "    dtree = DecisionTreeRegressor(max_depth=3)\n",
    "\n",
    "    dtree.fit(X_train, y_train) \n",
    "\n",
    "    # Define a bagging regressor with decision tree models\n",
    "    bagging_model = BaggingRegressor(base_estimator=dtree, n_estimators=10, random_state=42)\n",
    "\n",
    "    #Approach is to use the same training algorithm for every predictor and train them on different random subsets of the training set\n",
    "\n",
    "    # Define a voting regressor with decision tree models\n",
    "    voting_model = VotingRegressor([('tree1', dtree), ('tree2', dtree), ('tree3', dtree)])\n",
    "\n",
    "    # Fit the models on the training data\n",
    "    dtree.fit(X_train, y_train)\n",
    "    bagging_model.fit(X_train, y_train)\n",
    "    voting_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the testing data\n",
    "    y_pred_dtree = dtree.predict(X_test)\n",
    "    y_pred_bagging = bagging_model.predict(X_test)\n",
    "    y_pred_voting = voting_model.predict(X_test)\n",
    "    \n",
    "    mae_dtree = mean_absolute_error(y_test, y_pred_dtree)\n",
    "    mae_bagging = mean_absolute_error(y_test, y_pred_bagging)\n",
    "    mae_voting = mean_absolute_error(y_test, y_pred_voting)\n",
    "\n",
    "    # Calculate the mean squared error of the predictions\n",
    "    mse_dtree = mean_squared_error(y_test, y_pred_dtree)\n",
    "    mse_bagging = mean_squared_error(y_test, y_pred_bagging)\n",
    "    mse_voting = mean_squared_error(y_test, y_pred_voting)\n",
    "\n",
    "    rmse_dtree = np.sqrt(mse_dtree)\n",
    "    rmse_bag= np.sqrt(mse_bagging)\n",
    "    rmse_voting = np.sqrt(mse_voting)\n",
    "\n",
    "    R2_dtree = r2_score(y_test, y_pred_dtree)\n",
    "    R2_bagging = r2_score(y_test, y_pred_bagging)\n",
    "    R2_voting = r2_score(y_test, y_pred_voting)\n",
    "\n",
    "    tree = (mae_dtree, mse_dtree, rmse_dtree, R2_dtree)\n",
    "    tree_bag = (mae_bagging, mse_bagging, rmse_bag, R2_bagging)\n",
    "    tree_vot = (mae_voting, mse_voting, rmse_bag, R2_voting)\n",
    "\n",
    "    # Cross validation , cv on Decision tree model \n",
    "    \n",
    "    # Model 4 - Random Forest\n",
    "\n",
    "    # Initialize the model\n",
    "    rf = RandomForestRegressor(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=42)\n",
    "\n",
    "    #training model to Rfor.\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = rf.predict(X_test)\n",
    "\n",
    "    # evaluate the model performance using mean absolute error and mean squared error and RMSE\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    rf = (mae, mse, rmse, r2)\n",
    "          \n",
    "    # Model 5 - K-nearest neighbors\n",
    "    \n",
    "    # Train the KNN model\n",
    "    k = 5\n",
    "    knn = neighbors.KNeighborsRegressor(n_neighbors =k)\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    scaler=RobustScaler()\n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = knn.predict(X_test)\n",
    "\n",
    "    #Model performance metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    knn = (mae, mse, rmse, r2)\n",
    "\n",
    "    #if model performs good and generalises good , why negativ R2??\n",
    "    \n",
    "    # Model 6 - Lasso Regression\n",
    "    # Lasso regression can help with feature selection by shrinking the coefficients of less important features to zero.\n",
    "\n",
    "    # scaling the data\n",
    "    scaler = RobustScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # initialize lasso regression model\n",
    "    lasso = Lasso(alpha=0.1)\n",
    "\n",
    "    # fit the model to the training data\n",
    "    lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # make predictions on the test set\n",
    "    y_pred = lasso.predict(X_test_scaled)\n",
    "\n",
    "    # evaluate the model performance using mean absolute error and mean squared error and RMSE\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    RMSE = np.sqrt(mse)\n",
    "    R2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    lasso_reg = (mae, mse, RMSE, R2)\n",
    "    \n",
    "    # Model 7 - Polynomial\n",
    "    # Drop target column in df\n",
    "    X = df.drop(target_col, axis=1)\n",
    "    y = df[target_col]\n",
    "\n",
    "    # Convert X to a numpy array before reshaping\n",
    "    X_array = X.values.reshape(-1, 1)\n",
    "\n",
    "    poly= PolynomialFeatures(degree=2, include_bias= False)\n",
    "\n",
    "    X_poly = poly.fit_transform(X_array)\n",
    "\n",
    "    #print(X_poly.shape)\n",
    "\n",
    "    #splitting train, test val_set:\n",
    "    # train set=0.8, test set=0.2, val set= 0.8*0.25= 0.2\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "    # scaling w RobustScaler object and fit only to training data\n",
    "    scaler = RobustScaler()\n",
    "\n",
    "    # Apply scaler to training, validation, and test data\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "    lr.intercept_, lr.coef_ \n",
    "\n",
    "    y_pred_val = lr.predict(X_val_scaled)\n",
    "    y_pred_test = lr.predict(X_test_scaled)\n",
    "\n",
    "    #predictions on val set\n",
    "    mse = mean_squared_error(y_val, y_pred_val)\n",
    "    RMSE = np.sqrt(mse)\n",
    "\n",
    "    #print(\"Validation set MSE: {:.2f}\".format(mse))\n",
    "    #print(f'RMSE: {RMSE}')\n",
    "\n",
    "    #predictions on test set\n",
    "    mse = mean_squared_error(y_test, y_pred_test)\n",
    "    RMSE = np.sqrt(mse)\n",
    "\n",
    "    #print(\"Test set MSE: {:.2f}\".format(mse))\n",
    "    #print(f'RMSE: {RMSE}')\n",
    "\n",
    "    # Visualising the Polynomial Regression: edit\n",
    "\n",
    "    # got error code \n",
    "    #plt.scatter(X_array, y, color = 'blue')\n",
    "\n",
    "    #plt.plot(X, lg.predict(poly.fit_transform(X)), color = 'violet')\n",
    "    #plt.title('Polynomial Regression')\n",
    "    #plt.xlabel('X')\n",
    "    #plt.ylabel('ploy_predicted')\n",
    "    \n",
    "    pol = (\"N/A\", mse, RMSE, \"N/A\")\n",
    "    \n",
    "\n",
    "    models = lin_reg, svr, tree, tree_bag, tree_vot, rf, knn, lasso_reg, pol\n",
    "    # create a dictionary of data\n",
    "    headers = ['Model', 'Mae', 'Mse', 'RMSE', 'R2']\n",
    "    models_name = ['Linear Reg', 'SVR', 'Descion Tree', 'DT bag', 'DT vot', 'RF', 'KNN', 'Lasso', 'Polynomial']\n",
    "\n",
    "    data = {headers[0]: [models_name[0], models_name[1], models_name[2], models_name[3], models_name[4], models_name[5], models_name[6], models_name[7], models_name[8]],\n",
    "            headers[1]: [models[0][0], models[1][0], models[2][0], models[3][0], models[4][0], models[5][0], models[6][0], models[7][0], models[8][0]],\n",
    "            headers[2]: [models[0][1], models[1][1], models[2][1], models[3][1], models[4][1], models[5][1], models[6][1], models[7][1], models[8][1]],\n",
    "            headers[3]: [models[0][2], models[1][2], models[2][2], models[3][2], models[4][2], models[5][2], models[6][2], models[7][2], models[8][2]],\n",
    "            headers[4]: [models[0][3], models[1][3], models[2][3], models[3][3], models[4][3], models[5][3], models[6][3], models[7][3], models[8][3]]}\n",
    "\n",
    "    # create a DataFrame from the dictionary\n",
    "    df_models = pd.DataFrame(data)\n",
    "    \n",
    "    return df_models\n",
    "\n",
    "# Compare models and print result    \n",
    "df=pd.read_csv('./data/mvoies_processed_noTitle.csv')\n",
    "target_col = \"Rating\"\n",
    "\n",
    "models = compare_models(df, target_col)\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436c8705",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
